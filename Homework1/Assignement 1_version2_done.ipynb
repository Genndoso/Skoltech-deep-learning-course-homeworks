{"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"gFe273vKFgF-"},"source":["# Assignment 1 (60 points total)\n","\n","You will train a convolutional neural network (aka ConvNet or CNN) to solve yet another image classification problem: the Tiny ImageNet dataset (200 classes, 100K training images, 10K validation images). Try to achieve as high accuracy as possible.\n","\n","This exercise is close to what people do in real life. No toy architectures this time."]},{"cell_type":"markdown","metadata":{"id":"-S-WyYK8FgGF"},"source":["## Grading\n","\n","* 11 points for the report.\n","* 5 points for using an **interactive** (don't reinvent the wheel with `plt.plot`) tool for viewing progress, for example TensorBoard.\n","* 9 points for a network that gets $\\geq$25% accuracy on the private **test** set.\n","* Up to 35 points for accuracy up to 50%, issued linearly (i.e. 0 points for 25%, 7 points for 30%, 21 points for 40%, 35 points for $\\geq$50%.\n","\n","## Grading Explained\n","\n","* *Private test set*: it's a part of the dataset like the validation set, but for which the ground truth labels are known only to us (you won't be able to evaluate your model on it). When grading, we will compute test accuracy by running your code that computes val accuracy, but having replaced the images in `'val/'` with the test set.\n","* *How to submit*:\n","  * **<font color=\"red\">Read this in advance, don't leave until the last minute.</font> Wrong checkpoint submission = <font color=\"red\">0 points for accuracy</font>. Be careful!**\n","  * After you've trained your network, [save weights](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html) to \"*checkpoint.pth*\" with `model.state_dict()` and `torch.save()`.\n","  * Set `DO_TRAIN = False`, click \"Restart and Run All\" and make sure that your validation accuracy is computed correctly.\n","  * Compute the MD5 checksum for \"*checkpoint.pth*\" (e.g. run `!md5sum checkpoint.pth`) and paste it into \"*solution.py*\" (`get_checkpoint_metadata()`). You'll be penalized if this checksum doesn't match your submitted file.\n","  * Upload \"*checkpoint.pth*\" to Google Drive, copy the view-only link to it and paste it into \"*solution.py*\" as well.\n","  * Make sure \"Restart and Run All\" also works with `DO_TRAIN = True`, trains your model and saves `checkpoint.pth`.\n","  * <font color=\"red\">Important</font>: At least several hours before the deadline, **upload \"*solution.py*\" [here](http://350e-83-69-192-100.ngrok.io/) and make sure you get a \"üëå\"**.\n","\n","* *Report*: PDF, free form; should mention:\n","  * Your history of tweaks and improvements. How you started, what you searched. (*I have analyzed these and those conference papers/sources/blog posts. I tried this and that to adapt them to my problem. ...*)\n","  * Which network architectures have you tried? Which of them didn't work, and can you guess why? What is the final one and why?\n","  * Same for the training method (batch size, optimization algorithm, number of iterations, ...): which and why?\n","  * Same for anti-overfitting (regularization) techniques. Which ones have you tried? What were their effects, and can you guess why?\n","  * **Most importantly**: deep learning insights you gained. Can you give several examples of how *exactly* experience from this exercise will affect you training your future neural nets? (tricks, heuristics, conclusions, observations)\n","  * **List all sources of code**.\n","* *Progress viewing tool*: support the report with screenshots of accuracy and loss plots (training and validation) over time.\n","\n","## Restrictions\n","\n","* No pretrained networks.\n","* Don't enlarge images (e.g. don't resize them to $224 \\times 224$ or $256 \\times 256$).\n","\n","## Tips\n","\n","* **One change at a time**: don't test several new things at once (unless you are super confident that they will work). Train a model, introduce one change, train again.\n","* Google a lot: try to reinvent as few wheels as possible. Harvest inspiration from PyTorch recipes, from GitHub, from blogs...\n","* Use GPU.\n","* Regularization is very important: L2, batch normalization, dropout, data augmentation...\n","* Pay much attention to accuracy and loss graphs (e.g. in TensorBoard). Track failures early, stop bad experiments early.\n","* 2-3 hours of training (in Colab) should be enough for most models, maybe 4-6 hours if you're experimenting.\n","* Save checkpoints every so often in case things go wrong (optimization diverges, Colab disconnects...).\n","* Don't use too large batches, they can be slow and memory-hungry. This is true for inference too.\n","* Also don't forget to use `torch.no_grad()` and `.eval()` during inference."]},{"cell_type":"code","source":["!md5sum model.pth"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nsl2J7jpb_eY","executionInfo":{"status":"ok","timestamp":1648838584592,"user_tz":-180,"elapsed":8,"user":{"displayName":"–ú–∞–∫—Å–∏–º–∏–ª–∏–∞–Ω –ë—É—Ä–û–ø—Ç–∏–º–∞","userId":"12487926668048454689"}},"outputId":"37ee7b26-c8ef-44f2-a9a2-03a5acdba001"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["md5sum: model.pth: No such file or directory\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29540,"status":"ok","timestamp":1648838622592,"user":{"displayName":"–ú–∞–∫—Å–∏–º–∏–ª–∏–∞–Ω –ë—É—Ä–û–ø—Ç–∏–º–∞","userId":"12487926668048454689"},"user_tz":-180},"id":"tFM8olIJBrnF","outputId":"1aa0bfbe-f030-4c9f-81be-d82364e114ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Determine the locations of auxiliary libraries and datasets.\n","# `AUX_DATA_ROOT` is where 'tiny-imagenet-2022.zip' is.\n","\n","# Detect if we are in Google Colaboratory\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","except ImportError:\n","    IN_COLAB = False\n","\n","from pathlib import Path\n","if IN_COLAB:\n","    google.colab.drive.mount(\"/content/drive\")\n","    \n","    # Change this if you created the shortcut in a different location\n","    AUX_DATA_ROOT = Path(\"/content/drive/MyDrive/DL HW1\")\n","    # /content/drive/MyDrive/DL HW1/tiny-imagenet-2020.zip\n","    assert AUX_DATA_ROOT.is_dir(), \"Have you forgot to 'Add a shortcut to Drive'?\"\n","    \n","    import sys\n","    sys.path.append(str(AUX_DATA_ROOT))\n","else:\n","    AUX_DATA_ROOT = Path(\".\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JaBS9dpemjjx"},"outputs":[],"source":["# Imports\n","import torchvision\n","import torch\n","from torchvision import transforms,datasets\n","import torch.nn as nn\n","from torch.utils.tensorboard import SummaryWriter\n","import os\n","from torchvision.utils import make_grid\n","from matplotlib.pyplot import imshow\n","from tqdm import tqdm\n","import numpy as np\n","from torch.optim import lr_scheduler\n","# Your solution\n","%load_ext autoreload\n","%autoreload 1\n","\n","\n","import solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5o-v5TMloPb"},"outputs":[],"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WC8j82fll4Ep"},"outputs":[],"source":["# #!unzip -qq '/content/drive/MyDrive/DL HW1/tiny-imagenet-2020.zip'\n","\n","# # Define main data directory\n","# DATA_DIR = '/content/drive/MyDrive/DL HW1/tiny-imagenet-200' # Original images come in shapes of [3,64,64]\n","\n","# # Define training and validation data paths\n","# TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n","# VALID_DIR = os.path.join(DATA_DIR, 'val')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X5eKWQv9wi3P"},"outputs":[],"source":["# If `True`, will train the model from scratch and validate it.\n","# If `False`, instead of training will load weights from './checkpoint.pth'.\n","# When grading, we will test both cases.\n","DO_TRAIN = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njk50aDoFgGT"},"outputs":[],"source":["# Put training and validation images in `./tiny-imagenet-200/train` and `./tiny-imagenet-200/val`:\n","if not Path(\"tiny-imagenet-200/train/class_000/00000.jpg\").is_file():\n","    import zipfile\n","    with zipfile.ZipFile(AUX_DATA_ROOT / 'tiny-imagenet-2020.zip', 'r') as archive:\n","        archive.extractall()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExsacKR9KnPL"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFqnb1-EFgGj"},"outputs":[],"source":["# Initialize dataloaders\n","train_dataloader = solution.get_dataloader(\"./tiny-imagenet-200/\", 'train',256)\n","val_dataloader   = solution.get_dataloader(\"./tiny-imagenet-200/\", 'val',256)\n","\n","\n","\n","# Initialize the raw model\n","model = solution.get_model().to(device)\n","\n","\n","writer = SummaryWriter()\n","images, labels = next(iter(train_dataloader))\n","images,labels = images.cuda(),labels.cuda()\n","grid = torchvision.utils.make_grid(images)\n","writer.add_image(\"images\", grid)\n","writer.add_graph(model, images)\n","writer.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9n7DyGcFgGq"},"outputs":[],"source":["PATH = \"/content/drive/MyDrive/DL HW1/model.pth\"\n","\n","if DO_TRAIN:\n","    # Train from scratch\n","   # optimizer = solution.get_optimizer(model)\n","      optimizer = solution.get_optimizer(model)\n","      #solution.train_on_tinyimagenet(train_dataloader, val_dataloader, model, optimizer)\n","      solution.train_on_tinyimagenet(train_dataloader, val_dataloader, model,PATH, optimizer,num_epochs=40)\n","else:\n","    # Download the checkpoint and initialize model weights from it\n","    import urllib\n","    import subprocess\n","\n","    penalize = False\n","\n","    # Get your link and checksum\n","    claimed_md5_checksum, google_drive_link = solution.get_checkpoint_metadata()\n","\n","    # Use your link to download \"checkpoint.pth\"\n","    !pip install -U gdown\n","    !gdown --id {urllib.parse.urlparse(google_drive_link).path.split('/')[-2]} -O model.pth\n","\n","    try:\n","        # Compute the actual checksum\n","        real_md5_checksum = subprocess.check_output(\n","            [\"md5sum\", \"model.pth\"]).decode().split()[0]\n","    except subprocess.CalledProcessError as err:\n","        # Couldn't download or the filename isn't \"checkpoint.pth\"\n","        print(f\"Wrong link or filename: {err}\")\n","        penalize = True\n","    else:\n","        # The trained checkpoint is different from the one submitted\n","        if real_md5_checksum != claimed_md5_checksum:\n","            print(\"Checksums differ! Late submission?\")\n","            penalize = True\n","\n","    if penalize:\n","        print(\"üî´ Prepare the penalizer! üî´\")\n","\n","    # Finally load weights\n","   # solution.load_weights(model, \"./model_resnet50.pth\")\n","    checkpoint = torch.load('./model.pth', map_location=device)\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","  "]},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir=runs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":821},"id":"OHiIRc6D_a58","executionInfo":{"status":"ok","timestamp":1648810788995,"user_tz":-180,"elapsed":10739,"user":{"displayName":"–ú–∞–∫—Å–∏–º–∏–ª–∏–∞–Ω –ë—É—Ä–û–ø—Ç–∏–º–∞","userId":"12487926668048454689"}},"outputId":"657f8706-36d0-439d-d6b8-a5104eccb333"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDJw8MokFxP9","executionInfo":{"status":"ok","timestamp":1648811063753,"user_tz":-180,"elapsed":1338,"user":{"displayName":"–ú–∞–∫—Å–∏–º–∏–ª–∏–∞–Ω –ë—É—Ä–û–ø—Ç–∏–º–∞","userId":"12487926668048454689"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d73a0a15-f179-4946-a13b-f32bcd5e919b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class / Ground truth class\n","157 / 157\n","024 / 024\n","141 / 141\n","020 / 020\n","011 / 011\n","145 / 024\n","007 / 007\n","171 / 171\n","092 / 152\n","090 / 090\n","138 / 175\n","192 / 192\n","137 / 137\n","073 / 073\n","021 / 126\n"]}],"source":["# Classify some validation samples\n","import torch\n","\n","example_batch, example_batch_labels = next(iter(val_dataloader))\n","model.eval()\n","with torch.no_grad():\n","  _, example_predicted_labels = solution.predict(model, example_batch).max(1)\n","\n","print(\"Predicted class / Ground truth class\")\n","for predicted, gt in list(zip(example_predicted_labels, example_batch_labels))[:15]:\n","    print(\"{:03d} / {:03d}\".format(predicted, gt))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_Qddecy7-uS","executionInfo":{"status":"ok","timestamp":1648811072655,"user_tz":-180,"elapsed":8906,"user":{"displayName":"–ú–∞–∫—Å–∏–º–∏–ª–∏–∞–Ω –ë—É—Ä–û–ø—Ç–∏–º–∞","userId":"12487926668048454689"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1a1f8d3-e06e-4d6b-babc-d6ed0a2a7e91"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation accuracy: 49.03%\n"]}],"source":["# Print validation accuracy\n","val_accuracy, _ = validate(val_dataloader, model)\n","val_accuracy *= 100\n","assert 1.5 <= val_accuracy <= 100.0\n","print(\"Validation accuracy: %.2f%%\" % val_accuracy)"]},{"cell_type":"code","source":[""],"metadata":{"id":"s6vhRbofdMk1"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Assignement 1_version2_done.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":0}